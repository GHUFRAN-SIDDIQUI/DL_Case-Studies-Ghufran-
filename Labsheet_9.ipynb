{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab sheet - 9 (04 - 05 APR 2025)"
      ],
      "metadata": {
        "id": "65PKpM6SPIYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Import Dependencies\n",
        "First, we will import the necessary libraries and modules."
      ],
      "metadata": {
        "id": "INtN2SBABBiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XGNjzfuTA9z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the MNIST Dataset\n",
        "We will use torchvision to load the MNIST dataset. You can normalize the data by dividing by 255 and apply transformations like ToTensor to convert images to tensors."
      ],
      "metadata": {
        "id": "Tq1aDrZuBL4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QaBWjJq5A8Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [0, 1] range\n",
        "])\n",
        "\n",
        "# Load MNIST training and test datasets\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader for batching\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "-rhH8IAg_p3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter) #common mistake is using dataiter.next() which doesn't work anymore\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[9])\n",
        "\n",
        "fig = plt.figure(figsize = (5,5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "id": "HxWPvKLGPQSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the RNN Model\n",
        "We will define a simple RNN architecture for digit classification. The RNN will take 28 rows of the 28x28 image as sequential input (each row treated as a time-step)."
      ],
      "metadata": {
        "id": "eFAbxkTqBQV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Model(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RNN_Model, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # Use the output of the last time-step\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "j2X8CR94_1Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM\n",
        "A common variant on the vanilla RNN is the Long-Short Term Memory (LSTM) RNN. Vanilla RNNs can be tough to train on long sequences due to vanishing and exploding gradients caused by repeated matrix multiplication. LSTMs solve this problem by replacing the simple update rule of the vanilla RNN with a gating mechanism as follows.\n",
        "\n",
        "Similar to the vanilla RNN, at each timestep we receive an input $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$; the LSTM also maintains an $H$-dimensional *cell state*, so we also receive the previous cell state $c_{t-1}\\in\\mathbb{R}^H$. The learnable parameters of the LSTM are an *input-to-hidden* matrix $W_x\\in\\mathbb{R}^{4H\\times D}$, a *hidden-to-hidden* matrix $W_h\\in\\mathbb{R}^{4H\\times H}$ and a *bias vector* $b\\in\\mathbb{R}^{4H}$.\n",
        "\n",
        "At each timestep we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $g\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *block input* $g\\in\\mathbb{R}^H$ as\n",
        "\n",
        "$$\n",
        "i = \\sigma(a_i) \\hspace{2pc}\n",
        "f = \\sigma(a_f) \\hspace{2pc}\n",
        "o = \\sigma(a_o) \\hspace{2pc}\n",
        "g = \\tanh(a_g)\n",
        "$$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function and $\\tanh$ is the hyperbolic tangent, both applied elementwise.\n",
        "\n",
        "Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as\n",
        "\n",
        "$$\n",
        "c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc}\n",
        "h_t = o\\odot\\tanh(c_t)\n",
        "$$\n",
        "\n",
        "where $\\odot$ is the elementwise product of vectors.\n",
        "\n",
        "In the rest of the notebook we will implement the LSTM update rule and apply it to the image captioning task.\n",
        "\n",
        "In the code, we assume that data is stored in batches so that $X_t \\in \\mathbb{R}^{N\\times D}$ and will work with *transposed* versions of the parameters: $W_x \\in \\mathbb{R}^{D \\times 4H}$, $W_h \\in \\mathbb{R}^{H\\times 4H}$ so that activations $A \\in \\mathbb{R}^{N\\times 4H}$ can be computed efficiently as $A = X_t W_x + H_{t-1} W_h$"
      ],
      "metadata": {
        "id": "N_vIpNqnCKJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the LSTM Model\n",
        "Now, we will define the LSTM model, which will improve upon the RNN by addressing the vanishing gradient problem."
      ],
      "metadata": {
        "id": "BmbvwKgUBUg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Model(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_layers=2, num_classes=10):\n",
        "        super(LSTM_Model, self).__init__()\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size) -> (batch_size, 28, 28)\n",
        "\n",
        "        # LSTM expects input shape (batch, seq_length, input_size)\n",
        "        lstm_out, (hn, cn) = self.lstm(x)\n",
        "\n",
        "        # Use the last hidden state to predict the class\n",
        "        out = self.fc(hn[-1])\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "ijG_Os7q_14u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Evaluation Functions\n",
        "We will write the training and evaluation functions, which can be reused for both models."
      ],
      "metadata": {
        "id": "jUYUgiiUBYss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, trainloader, criterion, optimizer, num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "            # Flatten the input for RNN/LSTM (batch_size, seq_len, input_size)\n",
        "            inputs = inputs.view(-1, 28, 28)  # Reshaping for RNN or LSTM input\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}, Accuracy: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "id": "VOaTUY1u_4Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Function:"
      ],
      "metadata": {
        "id": "kDGT9-MIBcNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs = inputs.view(-1, 28, 28)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "id": "OAki3JZe_6sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training and Evaluation\n",
        "Now, let's train and evaluate both the RNN and LSTM models on the MNIST dataset."
      ],
      "metadata": {
        "id": "7lmOU567BgeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training RNN Model:"
      ],
      "metadata": {
        "id": "ZNrEF-JIBkv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer for RNN\n",
        "rnn_model = RNN_Model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the RNN model\n",
        "train(rnn_model, trainloader, criterion, optimizer, num_epochs=5)\n",
        "\n",
        "# Evaluate the RNN model\n",
        "evaluate(rnn_model, testloader)\n"
      ],
      "metadata": {
        "id": "MQie750N_9TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training LSTM Model:"
      ],
      "metadata": {
        "id": "CRAx6XvnBn0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ejfAWJtZFwKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer for LSTM\n",
        "lstm_model = LSTM_Model()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the LSTM model\n",
        "train(lstm_model, trainloader, criterion, optimizer, num_epochs=5)\n",
        "\n",
        "# Evaluate the LSTM model\n",
        "evaluate(lstm_model, testloader)\n"
      ],
      "metadata": {
        "id": "6lScaIeM__WT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}