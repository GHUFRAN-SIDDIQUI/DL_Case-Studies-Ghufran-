{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v--B9JaKMGzR"
      },
      "source": [
        "# Lab 11 (25-26 APRAIL 2025)\n",
        "#TOPICS COVERED : Variable Auto Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDGPAvKwjFQx"
      },
      "source": [
        "An autoencoder, also known as autoassociator or Diabolo networks, is an artificial neural network employed to recreate the given input. It takes a set of unlabeled inputs, encodes them and then tries to extract the most valuable information from them. They are used for feature extraction, learning generative models of data, dimensionality reduction and can be used for compression.\n",
        "\n",
        "Autoencoders are based on Restricted Boltzmann Machines, are employed in some of the largest deep learning applications. They are the building blocks of Deep Belief Networks (DBN).\n",
        "\n",
        "An autoencoder can be divided in two parts, the encoder and the decoder.\n",
        "\n",
        "The encoder needs to compress the representation of an input. In this case we are going to reduce the dimension. The decoder works like encoder network in reverse. It works to recreate the input, as closely as possible. This plays an important role during training, because it forces the autoencoder to select the most important features in the compressed representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egadoqpwjFQy"
      },
      "source": [
        "| **Feature**       | **CAEs**                        | **VAEs**                         |\n",
        "|--------------------|----------------------------------|-----------------------------------|\n",
        "| **Objective**      | Reconstruction                  | Generative modeling              |\n",
        "| **Latent Space**   | Deterministic                   | Probabilistic                    |\n",
        "| **Loss Function**  | Reconstruction only             | Reconstruction + KL Divergence   |\n",
        "| **Sampling**       | Not designed for sampling       | Latent space designed for sampling |\n",
        "| **Applications**   | Denoising, feature extraction   | Data generation, synthesis       |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "g4Lc-dVzLxUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2055c3-9936-4aff-d02e-c00ddcf71236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 546kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.45MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.00MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# load the training and test datasets\n",
        "train_data = datasets.MNIST(root='~/.pytorch/MNIST_data/', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='~/.pytorch/MNIST_data/', train=False,\n",
        "                                  download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xlp07WD7s3WM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "tI5pf86ALxUO"
      },
      "outputs": [],
      "source": [
        "# Create training and test dataloaders\n",
        "\n",
        "num_workers = 0\n",
        "# setting num_workers to a positive number would spawn that many data_loader processes, and use the multiple processes created to load the data. This way computation of the main code doesn't stop\n",
        "\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzma0LTLLxUP"
      },
      "source": [
        "### Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "NW-2T-_QLxUP",
        "outputId": "15f010da-f713-4724-a08d-86e96a3e73e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e9577bb2110>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAGsCAYAAAC8WvLKAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG/tJREFUeJzt3X9sVfX9x/FX+dELanu7Wtvbyg8LKGwgmDHoOhVhVEq3EX5tUeYS3IwG1xqViUvNFN3m6mA6w9YpfywwNwElGTDIwqbFlmwWDAgjxq2hpFvLaMtk672l2ILtZ3/s6/16baHc421P3+3zkXwSe+9593x2duPT214OSc45JwAAjBnm9wYAAPCCgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAk0b4vYGP6+rq0qlTp5SSkqKkpCS/twMA6EfOObW2tionJ0fDhl36PdaAC9ipU6c0duxYv7cBAPBRQ0ODxowZc8ljBtyPEFNSUvzeAgDAZ5fTggEXMH5sCAC4nBb0WcDKy8t13XXXadSoUcrLy9Nbb73VV6cCAAxBfRKwV155RatXr9batWv19ttva8aMGSosLNTp06f74nQAgKHI9YHZs2e74uLi6NednZ0uJyfHlZWV9TobDoedJBaLxWIN4RUOh3vtRcLfgZ0/f16HDx9WQUFB9LFhw4apoKBA1dXV3Y7v6OhQJBKJWQAA9CbhAXvvvffU2dmprKysmMezsrLU1NTU7fiysjIFg8Ho4iP0AIDL4funEEtLSxUOh6OroaHB7y0BAAxI+B9kzsjI0PDhw9Xc3BzzeHNzs0KhULfjA4GAAoFAorcBABjkEv4OLDk5WTNnzlRFRUX0sa6uLlVUVCg/Pz/RpwMADFF9ciup1atXa+XKlfrc5z6n2bNn6/nnn1dbW5u++c1v9sXpAABDUJ8E7I477tC//vUvPfHEE2pqatJNN92kvXv3dvtgBwAAXiU555zfm/ioSCSiYDDo9zYAAD4Kh8NKTU295DG+fwoRAAAvCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMGmE3xsALBg+fLinuWAwmOCdJF5JSYmnuSuuuCLumcmTJ3s6V3Fxsae5n/zkJ57mVqxYEfdMe3u7p3M988wznuaeeuopT3ODCe/AAAAmETAAgEkJD9iTTz6ppKSkmDVlypREnwYAMMT1ye/Apk6dqtdff/3/TzKCX7UBABKrT8oyYsQIhUKhvvjWAABI6qPfgR0/flw5OTmaMGGC7rrrLtXX11/02I6ODkUikZgFAEBvEh6wvLw8bd68WXv37tULL7yguro63XrrrWptbe3x+LKyMgWDwegaO3ZsorcEABiEEh6woqIife1rX9P06dNVWFio3//+92ppadGrr77a4/GlpaUKh8PR1dDQkOgtAQAGoT7/dEVaWppuuOEG1dbW9vh8IBBQIBDo620AAAaZPv9zYGfPntWJEyeUnZ3d16cCAAwhCQ/YI488oqqqKv3973/Xm2++qaVLl2r48OGebs0CAMDFJPxHiCdPntSKFSt05swZXXPNNbrlllt04MABXXPNNYk+FQBgCEt4wLZt25bobwkjxo0b52kuOTk57pkvfOELns51yy23eJpLS0vzNLd8+XJPc4PVyZMnPc1t2LDB09zSpUs9zV3sU9OX8pe//MXTuaqqqjzNgXshAgCMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMSnLOOb838VGRSETBYNDvbQxpN910k6e5ffv2eZrj/2+burq64p751re+5elcZ8+e9TTnVWNjY9wz//nPfzydq6amxtPcYBcOh5WamnrJY3gHBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwaYTfG8DAU19f72nuzJkznua4G32sgwcPeppraWnxNDdv3jxPc+fPn4975te//rWncwE94R0YAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAk7kaPbv797397mluzZo2nua985Stxzxw5csTTuTZs2OBpzqujR4/GPXP77bd7OldbW5unualTp3qae/DBBz3NAYnCOzAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmJTnnnN+b+KhIJKJgMOj3NtCPUlNT455pbW31dK6NGzd6mrvnnns8zX3jG9+Ie2br1q2ezgUMJuFwuNd/N/AODABgEgEDAJhEwAAAJsUdsP3792vRokXKyclRUlKSdu7cGfO8c05PPPGEsrOzNXr0aBUUFOj48eOJ2i8AAJI8BKytrU0zZsxQeXl5j8+vW7dOGzZs0IsvvqiDBw/qyiuvVGFhodrb2z/xZgEA+NCIeAeKiopUVFTU43POOT3//PP63ve+p8WLF0uSXnrpJWVlZWnnzp268847P9luAQD4Pwn9HVhdXZ2amppUUFAQfSwYDCovL0/V1dU9znR0dCgSicQsAAB6k9CANTU1SZKysrJiHs/Kyoo+93FlZWUKBoPRNXbs2ERuCQAwSPn+KcTS0lKFw+Hoamho8HtLAAADEhqwUCgkSWpubo55vLm5OfrcxwUCAaWmpsYsAAB6k9CA5ebmKhQKqaKiIvpYJBLRwYMHlZ+fn8hTAQCGuLg/hXj27FnV1tZGv66rq9PRo0eVnp6ucePG6aGHHtIPf/hDXX/99crNzdXjjz+unJwcLVmyJJH7BgAMcXEH7NChQ5o3b17069WrV0uSVq5cqc2bN+vRRx9VW1ub7rvvPrW0tOiWW27R3r17NWrUqMTtGgAw5HE3egwp69ev9zT34X+oxauqqirumY/+MZR4dHV1eZoDBiLuRg8AGLQIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJO4Gz2GlCuvvNLT3O7duz3N3XbbbXHPFBUVeTrXH//4R09zwEDE3egBAIMWAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASd6MHLsPEiRM9zb399ttxz7S0tHg61xtvvOFp7tChQ57mysvL454ZYP+6wQDG3egBAIMWAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASdzMF+hDS5cujXtm06ZNns6VkpLiac6rxx57LO6Zl156ydO5GhsbPc3BLm7mCwAYtAgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAk7gbPTDATJs2zdPcc88952lu/vz5nua82Lhxo6e5p59+2tPcP//5T09z8B93owcADFoEDABgEgEDAJgUd8D279+vRYsWKScnR0lJSdq5c2fM83fffbeSkpJi1sKFCxO1XwAAJHkIWFtbm2bMmKHy8vKLHrNw4UI1NjZG19atWz/RJgEA+LgR8Q4UFRWpqKjokscEAgGFQqHL+n4dHR3q6OiIfh2JROLdEgBgCOqT34FVVlYqMzNTkydP1v33368zZ85c9NiysjIFg8HoGjt2bF9sCQAwyCQ8YAsXLtRLL72kiooK/fjHP1ZVVZWKiorU2dnZ4/GlpaUKh8PR1dDQkOgtAQAGobh/hNibO++8M/rPN954o6ZPn66JEyeqsrKyxz8wGQgEFAgEEr0NAMAg1+cfo58wYYIyMjJUW1vb16cCAAwhfR6wkydP6syZM8rOzu7rUwEAhpC4f4R49uzZmHdTdXV1Onr0qNLT05Wenq6nnnpKy5cvVygU0okTJ/Too49q0qRJKiwsTOjGAQBDW9wBO3TokObNmxf9evXq1ZKklStX6oUXXtCxY8f0q1/9Si0tLcrJydGCBQv0gx/8gN9zAQASirvRA4NEWlqap7lFixZ5mtu0aVPcM0lJSZ7OtW/fPk9zt99+u6c5+I+70QMABi0CBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCTuRg/Ak46OjrhnRoyI+29wkiR98MEHnua8/j2ElZWVnuaQONyNHgAwaBEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJjk7c6aAPrM9OnTPc199atf9TQ3a9YsT3Neb8zrxbvvvutpbv/+/QneCQYS3oEBAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEzibvTAZZg8ebKnuZKSkrhnli1b5ulcoVDI01x/6uzs9DTX2Njoaa6rq8vTHGzgHRgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCTuRg+TvN55fcWKFZ7mvNxVXpKuu+46T3MWHDp0KO6Zp59+2tO5fve733maw+DGOzAAgEkEDABgUlwBKysr06xZs5SSkqLMzEwtWbJENTU1Mce0t7eruLhYV199ta666iotX75czc3NCd00AABxBayqqkrFxcU6cOCAXnvtNV24cEELFixQW1tb9JiHH35Yu3fv1vbt21VVVaVTp055/htmAQC4mLg+xLF3796Yrzdv3qzMzEwdPnxYc+bMUTgc1i9/+Utt2bJFX/ziFyVJmzZt0qc//WkdOHBAn//85xO3cwDAkPaJfgcWDoclSenp6ZKkw4cP68KFCyooKIgeM2XKFI0bN07V1dU9fo+Ojg5FIpGYBQBAbzwHrKurSw899JBuvvlmTZs2TZLU1NSk5ORkpaWlxRyblZWlpqamHr9PWVmZgsFgdI0dO9brlgAAQ4jngBUXF+udd97Rtm3bPtEGSktLFQ6Ho6uhoeETfT8AwNDg6Q8yl5SUaM+ePdq/f7/GjBkTfTwUCun8+fNqaWmJeRfW3Nx80T94GggEFAgEvGwDADCExfUOzDmnkpIS7dixQ/v27VNubm7M8zNnztTIkSNVUVERfaympkb19fXKz89PzI4BAFCc78CKi4u1ZcsW7dq1SykpKdHfawWDQY0ePVrBYFD33HOPVq9erfT0dKWmpuqBBx5Qfn4+n0AEACRUXAF74YUXJElz586NeXzTpk26++67JUk//elPNWzYMC1fvlwdHR0qLCzUL37xi4RsFgCAD8UVMOdcr8eMGjVK5eXlKi8v97wpAAB6w93okTBZWVme5j7zmc/EPfPzn//c07mmTJniac6CgwcPeppbv369p7ldu3bFPdPV1eXpXEBPuJkvAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAk7iZ7yCWnp7uaW7jxo2e5m666SZPcxMmTPA0Z8Gbb74Z98yzzz7r6Vx/+MMfPM29//77nuYAv/EODABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEnej72d5eXme5tasWRP3zOzZsz2d69prr/U0Z8G5c+c8zW3YsMHT3I9+9KO4Z9ra2jydCxhqeAcGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJu9H3s6VLl/brXH969913Pc3t2bMn7pkPPvjA07meffZZT3MtLS2e5gD0Hd6BAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMSnLOOb838VGRSETBYNDvbQAAfBQOh5WamnrJY3gHBgAwiYABAEyKK2BlZWWaNWuWUlJSlJmZqSVLlqimpibmmLlz5yopKSlmrVq1KqGbBgAgroBVVVWpuLhYBw4c0GuvvaYLFy5owYIFamtriznu3nvvVWNjY3StW7cuoZsGAGBEPAfv3bs35uvNmzcrMzNThw8f1pw5c6KPX3HFFQqFQonZIQAAPfhEvwMLh8OSpPT09JjHX375ZWVkZGjatGkqLS3VuXPnLvo9Ojo6FIlEYhYAAL1yHnV2drovf/nL7uabb455fOPGjW7v3r3u2LFj7je/+Y279tpr3dKlSy/6fdauXesksVgsFosVXeFwuNcOeQ7YqlWr3Pjx411DQ8Mlj6uoqHCSXG1tbY/Pt7e3u3A4HF0NDQ2+XzgWi8Vi+bsuJ2Bx/Q7sQyUlJdqzZ4/279+vMWPGXPLYvLw8SVJtba0mTpzY7flAIKBAIOBlGwCAISyugDnn9MADD2jHjh2qrKxUbm5urzNHjx6VJGVnZ3vaIAAAPYkrYMXFxdqyZYt27dqllJQUNTU1SZKCwaBGjx6tEydOaMuWLfrSl76kq6++WseOHdPDDz+sOXPmaPr06X3yPwAAMETF83svXeRnlZs2bXLOOVdfX+/mzJnj0tPTXSAQcJMmTXJr1qy5rJ9lfigcDvv+s1cWi8Vi+bsupxvczBcAMOBwM18AwKBFwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJg24gDnn/N4CAMBnl9OCARew1tZWv7cAAPDZ5bQgyQ2wtzxdXV06deqUUlJSlJSUFPNcJBLR2LFj1dDQoNTUVJ92OLBwTbrjmsTienTHNeluoFwT55xaW1uVk5OjYcMu/R5rRD/t6bINGzZMY8aMueQxqampvOg+hmvSHdckFtejO65JdwPhmgSDwcs6bsD9CBEAgMtBwAAAJpkKWCAQ0Nq1axUIBPzeyoDBNemOaxKL69Ed16Q7i9dkwH2IAwCAy2HqHRgAAB8iYAAAkwgYAMAkAgYAMImAAQBMMhWw8vJyXXfddRo1apTy8vL01ltv+b0l3zz55JNKSkqKWVOmTPF7W/1m//79WrRokXJycpSUlKSdO3fGPO+c0xNPPKHs7GyNHj1aBQUFOn78uD+b7Se9XZO7776722tm4cKF/my2H5SVlWnWrFlKSUlRZmamlixZopqamphj2tvbVVxcrKuvvlpXXXWVli9frubmZp923Pcu55rMnTu32+tk1apVPu340swE7JVXXtHq1au1du1avf3225oxY4YKCwt1+vRpv7fmm6lTp6qxsTG6/vSnP/m9pX7T1tamGTNmqLy8vMfn161bpw0bNujFF1/UwYMHdeWVV6qwsFDt7e39vNP+09s1kaSFCxfGvGa2bt3ajzvsX1VVVSouLtaBAwf02muv6cKFC1qwYIHa2tqixzz88MPavXu3tm/frqqqKp06dUrLli3zcdd963KuiSTde++9Ma+TdevW+bTjXjgjZs+e7YqLi6Nfd3Z2upycHFdWVubjrvyzdu1aN2PGDL+3MSBIcjt27Ih+3dXV5UKhkFu/fn30sZaWFhcIBNzWrVt92GH/+/g1cc65lStXusWLF/uyn4Hg9OnTTpKrqqpyzv3vNTFy5Ei3ffv26DF//etfnSRXXV3t1zb71ceviXPO3Xbbbe7BBx/0b1NxMPEO7Pz58zp8+LAKCgqijw0bNkwFBQWqrq72cWf+On78uHJycjRhwgTdddddqq+v93tLA0JdXZ2amppiXi/BYFB5eXlD+vUiSZWVlcrMzNTkyZN1//3368yZM35vqd+Ew2FJUnp6uiTp8OHDunDhQszrZMqUKRo3btyQeZ18/Jp86OWXX1ZGRoamTZum0tJSnTt3zo/t9WrA3Y2+J++99546OzuVlZUV83hWVpb+9re/+bQrf+Xl5Wnz5s2aPHmyGhsb9dRTT+nWW2/VO++8o5SUFL+356umpiZJ6vH18uFzQ9HChQu1bNky5ebm6sSJE3rsscdUVFSk6upqDR8+3O/t9amuri499NBDuvnmmzVt2jRJ/3udJCcnKy0tLebYofI66emaSNLXv/51jR8/Xjk5OTp27Ji++93vqqamRr/97W993G3PTAQM3RUVFUX/efr06crLy9P48eP16quv6p577vFxZxio7rzzzug/33jjjZo+fbomTpyoyspKzZ8/38ed9b3i4mK98847Q+r3xL252DW57777ov984403Kjs7W/Pnz9eJEyc0ceLE/t7mJZn4EWJGRoaGDx/e7dNBzc3NCoVCPu1qYElLS9MNN9yg2tpav7fiuw9fE7xeLm3ChAnKyMgY9K+ZkpIS7dmzR2+88UbM3zUYCoV0/vx5tbS0xBw/FF4nF7smPcnLy5OkAfk6MRGw5ORkzZw5UxUVFdHHurq6VFFRofz8fB93NnCcPXtWJ06cUHZ2tt9b8V1ubq5CoVDM6yUSiejgwYO8Xj7i5MmTOnPmzKB9zTjnVFJSoh07dmjfvn3Kzc2NeX7mzJkaOXJkzOukpqZG9fX1g/Z10ts16cnRo0claWC+Tvz+FMnl2rZtmwsEAm7z5s3u3Xffdffdd59LS0tzTU1Nfm/NF9/5zndcZWWlq6urc3/+859dQUGBy8jIcKdPn/Z7a/2itbXVHTlyxB05csRJcs8995w7cuSI+8c//uGcc+6ZZ55xaWlpbteuXe7YsWNu8eLFLjc3173//vs+77zvXOqatLa2ukceecRVV1e7uro69/rrr7vPfvaz7vrrr3ft7e1+b71P3H///S4YDLrKykrX2NgYXefOnYses2rVKjdu3Di3b98+d+jQIZefn+/y8/N93HXf6u2a1NbWuu9///vu0KFDrq6uzu3atctNmDDBzZkzx+ed98xMwJxz7mc/+5kbN26cS05OdrNnz3YHDhzwe0u+ueOOO1x2drZLTk521157rbvjjjtcbW2t39vqN2+88YaT1G2tXLnSOfe/j9I//vjjLisrywUCATd//nxXU1Pj76b72KWuyblz59yCBQvcNddc40aOHOnGjx/v7r333kH9H4A9XQtJbtOmTdFj3n//ffftb3/bfepTn3JXXHGFW7p0qWtsbPRv032st2tSX1/v5syZ49LT010gEHCTJk1ya9asceFw2N+NXwR/HxgAwCQTvwMDAODjCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADDpv/Iu97UOVZfjAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter) #common mistake is using dataiter.next() which doesn't work anymore\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (5,5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrCtCh4SOiWX"
      },
      "source": [
        "# **Variational Autoencoders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OZ11-HWJOhNw"
      },
      "outputs": [],
      "source": [
        "cuda = False\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "x_dim  = 784\n",
        "hidden_dim = 400\n",
        "latent_dim = 200\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wUPQwsYNMrEk"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
        "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
        "\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_       = self.LeakyReLU(self.FC_input(x))\n",
        "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
        "        mean     = self.FC_mean(h_)\n",
        "        log_var  = self.FC_var(h_)                     # encoder produces mean and log of variance\n",
        "                                                       #             (i.e., parameters of simple tractable normal distribution \"q\"\n",
        "\n",
        "        return mean, log_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-K36U2okODoA"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h     = self.LeakyReLU(self.FC_hidden(x))\n",
        "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
        "\n",
        "        x_hat = torch.sigmoid(self.FC_output(h))\n",
        "        return x_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sCEo5lQdODk0"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, Encoder, Decoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.Encoder = Encoder\n",
        "        self.Decoder = Decoder\n",
        "\n",
        "    def reparameterization(self, mean, var):\n",
        "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon\n",
        "        z = mean + var*epsilon                          # reparameterization trick\n",
        "        return z\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, log_var = self.Encoder(x)\n",
        "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
        "        x_hat            = self.Decoder(z)\n",
        "\n",
        "        return x_hat, mean, log_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K8rFb3tBOJX4"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
        "\n",
        "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c9Ol_hNeOL2P"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
        "\n",
        "    return reproduction_loss + KLD\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se9DmPlROPlf",
        "outputId": "e695de17-36ea-4c94-d22d-bd42e5888496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training VAE...\n",
            "\tEpoch 1 complete! \tAverage Loss:  30.481294247740784\n",
            "\tEpoch 2 complete! \tAverage Loss:  24.293703842035885\n",
            "\tEpoch 3 complete! \tAverage Loss:  23.079858647136938\n",
            "\tEpoch 4 complete! \tAverage Loss:  22.59851841157339\n",
            "\tEpoch 5 complete! \tAverage Loss:  22.273026685164584\n",
            "\tEpoch 6 complete! \tAverage Loss:  22.07943461378402\n",
            "\tEpoch 7 complete! \tAverage Loss:  21.926717364447004\n",
            "\tEpoch 8 complete! \tAverage Loss:  21.81391729494141\n",
            "\tEpoch 9 complete! \tAverage Loss:  21.70127719697256\n",
            "\tEpoch 10 complete! \tAverage Loss:  21.610856774035355\n",
            "\tEpoch 11 complete! \tAverage Loss:  21.549144196340187\n",
            "\tEpoch 12 complete! \tAverage Loss:  21.483923238491606\n",
            "\tEpoch 13 complete! \tAverage Loss:  21.437975729458646\n",
            "\tEpoch 14 complete! \tAverage Loss:  21.399593480216062\n",
            "\tEpoch 15 complete! \tAverage Loss:  21.33897024064988\n",
            "\tEpoch 16 complete! \tAverage Loss:  21.31469305930753\n",
            "\tEpoch 17 complete! \tAverage Loss:  21.28097912561023\n",
            "\tEpoch 18 complete! \tAverage Loss:  21.257314600065257\n",
            "\tEpoch 19 complete! \tAverage Loss:  21.23625011071399\n",
            "\tEpoch 20 complete! \tAverage Loss:  21.206494228544933\n",
            "\tEpoch 21 complete! \tAverage Loss:  21.181439759705057\n",
            "\tEpoch 22 complete! \tAverage Loss:  21.155558672959344\n",
            "\tEpoch 23 complete! \tAverage Loss:  21.1366256318024\n",
            "\tEpoch 24 complete! \tAverage Loss:  21.118904726868593\n",
            "\tEpoch 25 complete! \tAverage Loss:  21.11830459152074\n",
            "\tEpoch 26 complete! \tAverage Loss:  21.082947732658933\n",
            "\tEpoch 27 complete! \tAverage Loss:  21.08381809235891\n",
            "\tEpoch 28 complete! \tAverage Loss:  21.066136446930877\n",
            "\tEpoch 29 complete! \tAverage Loss:  21.062261674301592\n",
            "\tEpoch 30 complete! \tAverage Loss:  21.038666178521094\n"
          ]
        }
      ],
      "source": [
        "print(\"Start training VAE...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    overall_loss = 0\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        x = x.view(-1, 784)\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, mean, log_var = model(x)\n",
        "        loss = loss_function(x, x_hat, mean, log_var)\n",
        "\n",
        "        overall_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_N0N9RhOXmq"
      },
      "source": [
        "Generating images from noise"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JmCBirmXswFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}